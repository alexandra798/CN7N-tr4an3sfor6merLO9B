"""Attention visualization placeholder.

This project keeps explanation tooling out of the critical runtime path.
Extend here to visualize patch/level attention (e.g., by returning attn weights
from the Transformer blocks in evaluation mode).
"""
def main():
    print("attn_viz.py: placeholder")

if __name__ == "__main__":
    main()
