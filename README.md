# LiTCVG-HFT:Lightweight Transformer with Cross-level Value Graph for High-Frequency Trading


[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![C++20](https://img.shields.io/badge/C%2B%2B-20-blue.svg)](https://isocpp.org/)

> **End-to-end deep learning system for microsecond-level limit order book prediction**  
> From research prototype to production-ready deployment, bridging academic innovation with industrial-grade performance

---

## ðŸ“Œ Project Overview

This project implements a complete high-frequency trading prediction system that analyzes limit order books (LOB) in real-time and forecasts future price movement directions. The system employs Python for model training and research, with C++ implementation for low-latency inference engine achieving production-grade performance.

### Key Features

- **ðŸ§  Novel Architecture**: LiT-CVG Transformer specifically designed for order book microstructure
- **âš¡ Ultra-Low Latency**: C++ inference engine with <500Âµs per prediction
- **ðŸŽ¯ Multi-Horizon Prediction**: Joint forecasting of short (10-step), medium (50-step), and long-term (100-step) price movements
- **ðŸ“Š End-to-End Pipeline**: From raw LOB data to real-time trading signals
- **ðŸ”¬ Interpretability**: Attention visualization, alpha decay analysis, information coefficient computation
- **ðŸš€ Production-Ready**: ONNX export, multi-threaded pipeline, performance monitoring

---

## ðŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     TRAINING PIPELINE (Python)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Raw LOB Data â†’ Feature Engineering â†’ Model Training        â”‚
â”‚  â”œâ”€ OFI Computation    â”œâ”€ CVMix Layer                      â”‚
â”‚  â”œâ”€ Normalization      â”œâ”€ Graph Attention                   â”‚
â”‚  â””â”€ Windowing          â””â”€ Multi-Horizon Heads               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ ONNX Export
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  INFERENCE ENGINE (C++/ONNX)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Market Data â†’ Feature Extraction â†’ Inference â†’ Signals     â”‚
â”‚  [Thread 1]    [Thread 2]           [Thread 3]              â”‚
â”‚  Ring Buffer â†’ Feature Queue â†’ Prediction Queue             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Training Framework** | PyTorch 2.0+ | Model development & training |
| **Inference Engine** | ONNX Runtime + C++20 | Production deployment |
| **Data Processing** | NumPy, Pandas | Feature engineering |
| **Visualization** | Matplotlib, Seaborn | Results analysis |
| **Build System** | CMake 3.20+ | C++ project management |
| **Testing** | pytest, Custom C++ tests | Quality assurance |

---

## ðŸŽ¯ Model Innovation: LiT-CVG Architecture

### Core Components

#### 1ï¸âƒ£ **CVMix (Cross-level Value Mixing)**
```python
# Innovation: Cross-price-level feature mixing + channel doubling
Input:  (B, T, L, C)     # L=20 price levels, C=7 feature channels
Output: (B, T, L, 2C)    # Channel doubled, preserving original + mixed features
```
- **Purpose**: Capture interactions between different price levels (e.g., spread compression, liquidity migration)
- **Implementation**: Learnable linear transformation across L dimension + GELU activation
- **Advantage**: Better preservation of price level structure compared to traditional CNNs

#### 2ï¸âƒ£ **TinyGraphSummary (Graph Attention Aggregation)**
```python
# Multi-head attention captures price level relationships
Query/Key/Value: (B, T, L, 2C) â†’ (B, T, heads, L, d_k)
Attention: (B, T, heads, L, L)  # Inter-level interaction matrix
Output: (B, T, g_dim)           # Aggregated graph embedding
```
- **Purpose**: Learn which price levels are most important for prediction (typically near best bid/ask)
- **Interpretability**: Attention weight visualization reveals market microstructure focus

#### 3ï¸âƒ£ **Patch Embedding (Spatiotemporal Patchification)**
```python
# Split (T,L) into (pTÃ—pL) patches
T=100, L=20 â†’ patches=(10, 2) with pT=10, pL=10
Each patch: pTÃ—pLÃ—2C â†’ d_model (learnable projection)
```
- **Purpose**: Reduce computational complexity while preserving local spatiotemporal patterns
- **Inspiration**: Adapted from ViT for LOB's spatiotemporal characteristics

#### 4ï¸âƒ£ **Multi-Horizon Prediction Heads**
```python
# Joint prediction across multiple time horizons
logits_10:  Predict price direction in next 10 steps (short-term)
logits_50:  Predict price direction in next 50 steps (medium-term)
logits_100: Predict price direction in next 100 steps (long-term)
```
- **Advantage**: Single model captures both short-term fluctuations and long-term trends
- **Alpha Decay**: Prediction capability across horizons decays exponentially, aligning with market efficiency hypothesis

### Complete Forward Pass
```
Input LOB (B,T,L,C=7)
    â†“ CVMix
(B,T,L,2C=14)
    â†“ Graph Attention
(B,T,g_dim=16) + (B,N,d_model=64)
    â†“ Transformer Blocks (depth=2)
(B,N,d_model)
    â†“ Pooling + Concat
(B, d_model+g_dim)
    â†“ Multi-Horizon Heads
logits_10, logits_50, logits_100 (each: BÃ—3)
```

---

## ðŸ”¬ Feature Engineering

### Raw LOB Features (per price level)
| Feature | Description | Formula |
|---------|-------------|---------|
| `bid_size` | Bid order quantity | - |
| `ask_size` | Ask order quantity | - |
| `OFI` | Order Flow Imbalance | `Î”bid_size - Î”ask_size` |
| `spread` | Bid-ask spread | `ask_p - bid_p` |
| `mid` | Mid-price | `(bid_p + ask_p) / 2` |
| `dmid` | Mid-price change | `mid_t - mid_{t-1}` |
| `cum_vol` | Cumulative volume | `âˆ‘ trade_vol` |

### Optional Advanced Features
```python
FeatureConfig(
    include_price=True,             # Normalized price levels
    include_volume_profile=True,    # Volume distribution
    ofi_levels=5                    # Multi-level OFI
)
```

### Data Normalization
- **Method**: Welford online algorithm for mean/std computation
- **Purpose**: Training stabilization and convergence acceleration
- **Storage**: JSON format for easy C++ loading

---

## âš¡ C++ Inference Engine

### Performance Metrics (Single-threaded, CPU)
| Component | P50 Latency | P99 Latency | Throughput |
|-----------|-------------|-------------|------------|
| Feature Extraction | 2.5 Âµs | 4.8 Âµs | 400K ops/s |
| Normalization | 0.8 Âµs | 1.2 Âµs | 1.2M ops/s |
| ONNX Inference | 350 Âµs | 520 Âµs | 2.8K infer/s |
| **End-to-End** | **360 Âµs** | **530 Âµs** | **2.7K pred/s** |

### Multi-threaded Pipeline Architecture
```cpp
Thread 1: Market Data Receiver
    â†“ RingBuffer(4096)
Thread 2: Feature Extraction + Normalization
    â†“ RingBuffer(2048)
Thread 3: Model Inference + Signal Generation
```

**Advantages**:
- âœ… Lock-free design minimizes thread contention
- âœ… Pipeline parallelism maximizes throughput
- âœ… Backpressure mechanism prevents memory overflow

### Key Optimization Techniques
1. **Memory Pool**: Pre-allocated ring buffers avoid dynamic allocation
2. **SIMD**: Feature computation leverages vectorized instructions
3. **Cache-Friendly**: Data structures aligned to cache lines
4. **Compiler Optimization**: `-O3 -march=native -flto`

---

## ðŸ“Š Experimental Results

### Dataset: FI-2010
- **Description**: Real LOB data from Finnish Stock Exchange
- **Scale**: 254,701 training samples, 139,488 test samples
- **Classes**: 3 classes (down, stationary, up)

### Performance Comparison (from original Kaggle notebook)
| Model | Test Accuracy | Parameters | Inference Speed |
|-------|--------------|------------|-----------------|
| CNN-LSTM-DeepLOB | 73.4% | ~500K | ~5ms |
| **LiT-CVG (This Project)** | **77.1%** | **197K** | **<0.5ms** |

### Alpha Decay Analysis
```
Horizon    IC (Spearman)   Half-life
10-step    0.285           ~45 steps
50-step    0.178           (exponential decay)
100-step   0.092           
```
- **Interpretation**: Short-term prediction strongest, aligning with market microstructure theory
- **Application**: Can be used to dynamically adjust holding periods

---

## ðŸš€ Quick Start

### Environment Setup

```bash
# Python environment
conda create -n litcvg python=3.10
conda activate litcvg
pip install torch numpy pandas matplotlib seaborn scikit-learn pyyaml

# C++ dependencies (macOS example)
brew install onnxruntime

# Or build from source (Linux)
git clone https://github.com/microsoft/onnxruntime
cd onnxruntime
./build.sh --config Release --parallel
```

### Training Model

```bash
# 1. Generate normalization statistics
python python0/train.py --config config/train.yaml --make-norm

# 2. Train model (supports FI-2010 or synthetic data)
python python0/train.py --config config/train.yaml

# 3. Export to ONNX
python python0/export_onnx.py \
    --config config/export.yaml \
    --ckpt artifacts/ckpt.pt \
    --out artifacts/model.onnx

# 4. Evaluate model
python python0/evaluate.py \
    --config config/export.yaml \
    --ckpt artifacts/ckpt.pt
```

### C++ Inference

```bash
# Compile
cd cpp && mkdir build && cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j$(nproc)

# Performance benchmark
./perf_benchmark artifacts/model.onnx artifacts/norm_stats.json

# Multi-threaded streaming inference
./main_stream_threaded ../config/runtime.yaml

# Visualize results
python python0/explain/visualize_perf.py
```

---

## ðŸ”¬ Interpretability & Analysis

### 1. Attention Visualization
```bash
python python0/explain/attn_viz.py \
    --config config/train.yaml \
    --ckpt artifacts/ckpt.pt
```
Generates:
- Price level attention heatmaps
- Temporal attention evolution plots
- Attention differences across prediction outcomes

### 2. Alpha Decay Analysis
```bash
python python0/explain/alpha_term.py \
    --config config/train.yaml \
    --ckpt artifacts/ckpt.pt
```
Outputs:
- Information Coefficient (IC) confidence intervals
- Half-life estimation
- IC stability analysis

### 3. Transaction Cost Analysis (TCA)
```cpp
// C++ implementation
TCAConfig cfg{
    .fee = 0.0001,           // 0.01% commission
    .slip_bps = 0.5,         // 0.5 bps slippage
    .half_spread = 0.005,    // 0.5Â¢ bid-ask spread
    .latency_us = 100        // 100Î¼s latency
};

TCAResult result = backtest_multi_horizon(
    signal_10, signal_50, signal_100, cfg
);
// Outputs: PnL, Sharpe ratio, turnover
```

---

## ðŸ“‚ Project Structure

```
litcvg-hft/
â”œâ”€â”€ python0/                    # Python training pipeline
â”‚   â”œâ”€â”€ data/                   # Data processing
â”‚   â”‚   â”œâ”€â”€ features.py        # Feature engineering (OFI, normalization)
â”‚   â”‚   â”œâ”€â”€ fi2010.py          # FI-2010 data loader
â”‚   â”‚   â”œâ”€â”€ loaders.py         # Data pipeline builder
â”‚   â”‚   â””â”€â”€ windows.py         # Sliding window
â”‚   â”œâ”€â”€ model/                  # Model definitions
â”‚   â”‚   â”œâ”€â”€ lit_cvg.py         # LiT-CVG architecture
â”‚   â”‚   â””â”€â”€ losses.py          # Multi-horizon loss functions
â”‚   â”œâ”€â”€ explain/                # Interpretability
â”‚   â”‚   â”œâ”€â”€ attn_viz.py        # Attention visualization
â”‚   â”‚   â””â”€â”€ alpha_term.py      # Alpha decay analysis
â”‚   â”œâ”€â”€ train.py               # Training script
â”‚   â”œâ”€â”€ export_onnx.py         # ONNX export
â”‚   â””â”€â”€ evaluate.py            # Model evaluation
â”œâ”€â”€ cpp/                        # C++ inference engine
â”‚   â”œâ”€â”€ include/               # Header files
â”‚   â”‚   â”œâ”€â”€ session.hpp        # ONNX Runtime wrapper
â”‚   â”‚   â”œâ”€â”€ preproc.hpp        # Feature preprocessing
â”‚   â”‚   â”œâ”€â”€ fe_ofi.hpp         # OFI computation
â”‚   â”‚   â”œâ”€â”€ ring_buffer.hpp    # Lock-free ring buffer
â”‚   â”‚   â””â”€â”€ perf_stats.hpp     # Performance statistics
â”‚   â”œâ”€â”€ src/                   # Implementation
â”‚   â”‚   â”œâ”€â”€ main_stream_threaded.cpp  # Multi-threaded inference
â”‚   â”‚   â”œâ”€â”€ perf_benchmark.cpp        # Performance testing
â”‚   â”‚   â””â”€â”€ tca.cpp            # Transaction cost analysis
â”‚   â””â”€â”€ CMakeLists.txt         # Build configuration
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ train.yaml             # Training parameters
â”‚   â”œâ”€â”€ export.yaml            # Export configuration
â”‚   â””â”€â”€ runtime.yaml           # Inference configuration
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ test_parity_*.py       # Python/C++ parity tests
â”‚   â””â”€â”€ latency_smoke.py       # Latency smoke tests
â””â”€â”€ artifacts/                  # Output directory
    â”œâ”€â”€ model.onnx             # Exported model
    â”œâ”€â”€ norm_stats.json        # Normalization parameters
    â”œâ”€â”€ ckpt.pt                # PyTorch checkpoint
    â”œâ”€â”€ plots/                 # Visualization results
    â””â”€â”€ perf/                  # Performance reports
```

---

## ðŸŽ“ Theoretical Foundation

### Market Microstructure
1. **Order Flow Imbalance (OFI)**: Key metric for measuring buy/sell pressure
   - Reference: Cont et al. (2014) "The Price Impact of Order Book Events"
   
2. **Price Discovery Mechanism**: How LOB reflects market information
   - Theory: Information asymmetry and adverse selection

3. **Alpha Decay**: Predictive signals decay over time
   - Formula: `IC(h) = ICâ‚€ Â· exp(-Î»h)`
   - Half-life: `tâ‚/â‚‚ = ln(2)/Î»`

### Deep Learning Methodology
1. **Transformer for Time Series**: 
   - Attention mechanism captures long-range dependencies
   - Patch embedding reduces complexity

2. **Multi-Task Learning**:
   - Joint optimization across horizons improves generalization
   - Adaptive weighting strategy

3. **Domain Adaptation**:
   - Cross-market transfer learning (future work)

---

## ðŸ“ˆ Future Work

### Short-term Plans
- [ ] Add reinforcement learning layer for execution optimization
- [ ] Support more market data sources (futures, options)
- [ ] GPU inference acceleration (CUDA/TensorRT)

### Medium-term Plans
- [ ] Multi-asset portfolio optimization
- [ ] Market impact model integration
- [ ] Real-time risk management

### Long-term Vision
- [ ] Complete trading system (OMS + risk + monitoring)
- [ ] Distributed backtesting framework
- [ ] Production-grade monitoring and alerting

---

## ðŸ¤ Technical Highlights (Interview Focus)

### 1. System Design Capability
- **End-to-End Thinking**: Complete pipeline from research to production
- **Performance Optimization**: Python prototype â†’ C++ production, 100Ã— speedup
- **Multi-threaded Architecture**: Lock-free ring buffer, zero-copy design

### 2. Algorithmic Innovation
- **CVMix Layer**: Novel design tailored for LOB structure
- **Multi-Horizon Prediction**: Capturing multiple market timescales simultaneously
- **Adaptive Loss**: Weighting strategy based on alpha decay theory

### 3. Engineering Practice
- **Code Quality**: Complete unit tests, parity tests
- **Maintainability**: Clear modular design, YAML config management
- **Scalability**: Pluggable data sources, features, models

### 4. Quantitative Finance Knowledge
- **Market Microstructure**: OFI, spread, liquidity
- **Transaction Cost Analysis**: Slippage, market impact modeling
- **Alpha Research**: IC analysis, half-life estimation

---

## ðŸ“š References

1. **DeepLOB**: Zhang et al. (2019) - "DeepLOB: Deep Convolutional Neural Networks for Limit Order Books"
2. **Transformer**: Vaswani et al. (2017) - "Attention Is All You Need"
3. **Vision Transformer**: Dosovitskiy et al. (2020) - "An Image is Worth 16x16 Words"
4. **Order Flow Imbalance**: Cont et al. (2014) - "The Price Impact of Order Book Events"
5. **FI-2010 Dataset**: Ntakaris et al. (2018) - "Benchmark Dataset for Mid-Price Forecasting"

---

## ðŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details

